{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.364781",
     "start_time": "2017-01-28T22:33:34.348792"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import ipdb\n",
    "#%pdb\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv, random, numpy as np\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import img_to_array, load_img, flip_axis, random_shift\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.382909",
     "start_time": "2017-01-28T22:33:34.367925"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "#Paths\n",
    "PATH_TRAIN_FOLDER = 'Training_Data/Udacity_Training_Data/'\n",
    "PATH_VALIDATION1 = 'Training_Data/VAL_TRACK1/'\n",
    "PATH_VALIDATION2 = 'Training_Data/VAL_TRACK2/'\n",
    "FILENAME_CSV = 'driving_log.csv'\n",
    "\n",
    "#Image \n",
    "IMAGE_CUT_TOP_HEIGHT = 55\n",
    "IMAGE_CUT_DOWN_HEIGHT = 25\n",
    "IMAGE_RESIZE_WIDTH = 64\n",
    "IMAGE_RESIZE_HEIGHT = 64\n",
    "\n",
    "#Camera\n",
    "CAMERA_LEFT_RIGHT_OFFSET = 0.2\n",
    "\n",
    "#Chances for Augmentation\n",
    "CHANCES_SHIFT = 0.5\n",
    "CHANCES_FLIP = 0.5\n",
    "CHANCES_DARKEN = 0.5\n",
    "BRIGHTNESS_RANGE = 0.3\n",
    "\n",
    "#Further Parameters\n",
    "SPEED_MINIMUM = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.408584",
     "start_time": "2017-01-28T22:33:34.385896"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read CSV\n",
    "def read_csv(path):\n",
    "    X, y = [], [] \n",
    "    \n",
    "    csv = pd.read_csv(path)\n",
    "    \n",
    "    #Throw away slow instances\n",
    "    csv = csv[(csv['speed']>SPEED_MINIMUM)]\n",
    "\n",
    "    for index, row in csv.iterrows():\n",
    "        #center\n",
    "        X.append(row['center'].strip())\n",
    "        y.append(row['steering'])\n",
    "        #left\n",
    "        X.append(row['left'].strip())\n",
    "        y.append(row['steering']+CAMERA_LEFT_RIGHT_OFFSET)\n",
    "        #right\n",
    "        X.append(row['right'].strip())\n",
    "        y.append(row['steering']-CAMERA_LEFT_RIGHT_OFFSET)\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.434873",
     "start_time": "2017-01-28T22:33:34.411824"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read Images\n",
    "\n",
    "def resize_and_normalize(img):\n",
    "    #Cutting the Top and the Bottom of the image\n",
    "    img_cut = img[IMAGE_CUT_TOP_HEIGHT:160-IMAGE_CUT_DOWN_HEIGHT, :, :]\n",
    "\n",
    "    #Resize to smaller image size\n",
    "    img_resize = cv2.resize(img_cut, (IMAGE_RESIZE_WIDTH, IMAGE_RESIZE_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "    #Normalizing to a range of -0.5 to +0.5\n",
    "    img_norm = (img_resize / 255. - .5).astype(np.float32)\n",
    "\n",
    "    return img_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.490441",
     "start_time": "2017-01-28T22:33:34.443947"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augmentation(path, steering, validation):    \n",
    "\n",
    "    #Load\n",
    "    image = cv2.imread(path)\n",
    "    \n",
    "    #Augment\n",
    "    if not validation:\n",
    "        #Darken\n",
    "        if random.random() < CHANCES_DARKEN:\n",
    "            image = random_darken(image)\n",
    "\n",
    "        #Shift\n",
    "        if random.random() < CHANCES_SHIFT:\n",
    "            image = random_shift(image, 0, 0.2, 0, 1, 2)\n",
    "\n",
    "        #Flip\n",
    "        if random.random() < CHANCES_FLIP:\n",
    "            image = flip_axis(image,1)\n",
    "            steering = steering * -1    \n",
    "            \n",
    "    #Resize\n",
    "    image = resize_and_normalize(image)\n",
    "\n",
    "    \n",
    "    return image, steering\n",
    "    \n",
    "def random_darken(image):\n",
    "    \n",
    "    w = image.shape[0]\n",
    "    h = image.shape[1]\n",
    "    \n",
    "    # Convert the image to HSV\n",
    "    temp = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Compute a random brightness value and apply to the image\n",
    "    brightness = BRIGHTNESS_RANGE + np.random.uniform()\n",
    "    \n",
    "    # Create a random Box\n",
    "    x1, y1 = random.randint(0, w), random.randint(0, h)\n",
    "    x2, y2 = random.randint(x1, w), random.randint(y1, h)\n",
    "    for i in range(x1, x2):\n",
    "        for j in range(y1, y2):\n",
    "            temp[i,j, 2] = temp[i, j, 2] * brightness\n",
    "\n",
    "    # Convert back to RGB and return\n",
    "    return cv2.cvtColor(temp, cv2.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.526208",
     "start_time": "2017-01-28T22:33:34.499077"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "def model(load, shape, checkpoint=None):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 5, 5, activation='elu', input_shape=shape))\n",
    "    model.add(MaxPooling2D())\n",
    "\n",
    "    model.add(Convolution2D(32, 5, 5, activation='elu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Convolution2D(32, 5, 5, activation='elu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Convolution2D(64, 3, 3, activation='elu'))\n",
    "    model.add(MaxPooling2D())\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(1024, activation='elu'))\n",
    "    model.add(Dense(512, activation='elu'))\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NVIDIA-Model\n",
    "def NVIDIA_model(load, shape, checkpoint=None):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(24, 5, 5,subsample=(2, 2), activation='elu', input_shape=shape))\n",
    "    model.add(Convolution2D(36, 5, 5,subsample=(2, 2), activation='elu'))\n",
    "    model.add(Convolution2D(48, 5, 5,subsample=(2, 2), activation='elu'))\n",
    "    model.add(Convolution2D(64, 3, 3,activation='elu'))\n",
    "    model.add(Convolution2D(64, 3, 3,activation='elu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=\"adam\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the [NVIDIA-Architecture](http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf):\n",
    "![image alt >](res/nvidia.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-28T21:33:34.373Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _generator(batch_size, X, y, path, validation=False):    \n",
    "    while 1:\n",
    "        batch_X, batch_y = [], []\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            sample_index = random.randint(0, len(X) - 1)\n",
    "            sa = y[sample_index]       \n",
    "            \n",
    "            image, sa = augmentation(path+X[sample_index], sa, validation)\n",
    "            batch_X.append(image)\n",
    "            batch_y.append(sa)\n",
    "        yield np.array(batch_X), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(net):\n",
    "    net.save('model.h5')\n",
    "    \n",
    "    json_string = net.to_json()\n",
    "    with open('model.json', 'w') as outfile:\n",
    "        outfile.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net,X, y, path):\n",
    "\n",
    "    net.fit_generator(_generator(256, X, y, path), samples_per_epoch=21990, nb_epoch=8)\n",
    "    save_model(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(net,X, y, path):\n",
    "    return net.evaluate_generator(_generator(256, X, y, path, validation=True), val_samples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_drivinig():\n",
    "    #Build model\n",
    "    net = model(load=False, shape=(IMAGE_RESIZE_WIDTH, IMAGE_RESIZE_HEIGHT, 3))\n",
    "    \n",
    "    #Read data and train test split them\n",
    "    X,y = read_csv(PATH_TRAIN_FOLDER+FILENAME_CSV)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    #Training\n",
    "    train(net, X_train, y_train, PATH_TRAIN_FOLDER)\n",
    "    \n",
    "    #Evaluation - Validation\n",
    "    loss = evaluate(net, X_test, y_test, PATH_TRAIN_FOLDER)\n",
    "    print(\"Evaluation - Validation: {}\".format(loss))\n",
    "    \n",
    "    #Evaluation - Testset #1\n",
    "    X_test,y_test = read_csv(PATH_VALIDATION1+FILENAME_CSV)\n",
    "    loss = evaluate(net, X_test, y_test, PATH_VALIDATION1)\n",
    "    print(\"Evaluation - Test#1: {}\".format(loss))\n",
    "        \n",
    "    #Evaluation - Testset#2\n",
    "    X_test,y_test = read_csv(PATH_VALIDATION2+FILENAME_CSV)\n",
    "    loss = evaluate(net, X_test, y_test, PATH_VALIDATION2)\n",
    "    print(\"Evaluation - Test#2: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_checker():\n",
    "    print(\"*------- STANDARD ---------*\")\n",
    "    learn_drivinig()\n",
    "    \n",
    "    #Camera\n",
    "    CAMERA_LEFT_RIGHT_OFFSET = 0.1\n",
    "    print(\"*------- CAMERA_LEFT_RIGHT_OFFSET = 0.1 ---------*\")\n",
    "    learn_drivinig()\n",
    "    \n",
    "    #Camera\n",
    "    CAMERA_LEFT_RIGHT_OFFSET = 0.3\n",
    "    print(\"*------- CAMERA_LEFT_RIGHT_OFFSET = 0.3 ---------*\")\n",
    "    learn_drivinig()\n",
    "    \n",
    "    #resest\n",
    "    CAMERA_LEFT_RIGHT_OFFSET = 0.2\n",
    "    \n",
    "    #Image \n",
    "    IMAGE_CUT_TOP_HEIGHT = 65\n",
    "    IMAGE_CUT_DOWN_HEIGHT = 35 \n",
    "    print(\"*------- IMAGE_CUT_TOP_HEIGHT = 65 \\nIMAGE_CUT_DOWN_HEIGHT = 35  ---------*\")\n",
    "    learn_drivinig()\n",
    "    \n",
    "    #Image \n",
    "    IMAGE_CUT_TOP_HEIGHT = 45\n",
    "    IMAGE_CUT_DOWN_HEIGHT = 15 \n",
    "    print(\"*------- IMAGE_CUT_TOP_HEIGHT = 45 \\nIMAGE_CUT_DOWN_HEIGHT = 15 ---------*\")\n",
    "    learn_drivinig()  \n",
    "    \n",
    "    #reset\n",
    "    IMAGE_CUT_TOP_HEIGHT = 55\n",
    "    IMAGE_CUT_DOWN_HEIGHT = 25 \n",
    "    \n",
    "    IMAGE_RESIZE_WIDTH = 64\n",
    "    IMAGE_RESIZE_HEIGHT = 64\n",
    "    print(\"*------- IMAGE_RESIZE_WIDTH = 64 \\nIMAGE_RESIZE_HEIGHT = 64 ---------*\")\n",
    "    learn_drivinig()  \n",
    "    \n",
    "    IMAGE_RESIZE_WIDTH = 32\n",
    "    IMAGE_RESIZE_HEIGHT = 32\n",
    "    print(\"*------- IMAGE_RESIZE_WIDTH = 32 \\nIMAGE_RESIZE_HEIGHT = 32 ---------*\")\n",
    "    learn_drivinig()  \n",
    "    \n",
    "    #reset\n",
    "    IMAGE_RESIZE_WIDTH = 100\n",
    "    IMAGE_RESIZE_HEIGHT = 100\n",
    "    \n",
    "    #NoAugmentation\n",
    "    CHANCES_SHIFT = 0.0\n",
    "    CHANCES_FLIP = 0.0\n",
    "    CHANCES_DARKEN = 0.0\n",
    "    print(\"*------- No Augmentation ---------*\")\n",
    "    learn_drivinig()  \n",
    "    \n",
    "    #reset\n",
    "    CHANCES_SHIFT = 0.5\n",
    "    CHANCES_FLIP = 0.5\n",
    "    CHANCES_DARKEN = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-28T21:33:34.380Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':   \n",
    "    #parameter_checker()\n",
    "    learn_drivinig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('model.json', 'r') as jfile:\n",
    "          model = model_from_json(jfile.read())\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_model = model(load=False, shape=(100,100,3))\n",
    "n_model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
