{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.364781",
     "start_time": "2017-01-28T22:33:34.348792"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import ipdb\n",
    "#%pdb\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv, random, numpy as np\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import img_to_array, load_img, flip_axis, random_shift\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.382909",
     "start_time": "2017-01-28T22:33:34.367925"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "#Paths\n",
    "PATH_TRAIN_FOLDER = 'Training_Data/Udacity_Training_Data/'\n",
    "PATH_VALIDATION1 = 'Training_Data/VAL_TRACK1/'\n",
    "PATH_VALIDATION2 = 'Training_Data/VAL_TRACK2/'\n",
    "FILENAME_CSV = 'driving_log.csv'\n",
    "\n",
    "#Image \n",
    "IMAGE_CUT_TOP_HEIGHT = 55\n",
    "IMAGE_CUT_DOWN_HEIGHT = 25\n",
    "IMAGE_RESIZE_WIDTH = 100\n",
    "IMAGE_RESIZE_HEIGHT = 100\n",
    "\n",
    "#Camera\n",
    "CAMERA_LEFT_RIGHT_OFFSET = 0.2\n",
    "\n",
    "#Chances for Augmentation\n",
    "CHANCES_SHIFT = 0.5\n",
    "CHANCES_FLIP = 0.5\n",
    "CHANCES_DARKEN = 0.5\n",
    "BRIGHTNESS_RANGE = 0.3\n",
    "\n",
    "#Further Parameters\n",
    "SPEED_MINIMUM = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.408584",
     "start_time": "2017-01-28T22:33:34.385896"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read CSV\n",
    "def read_csv(path):\n",
    "    X, y = [], [] \n",
    "    \n",
    "    csv = pd.read_csv(path)\n",
    "    \n",
    "    #Throw away slow instances\n",
    "    csv = csv[(csv['speed']>SPEED_MINIMUM)]\n",
    "\n",
    "    for index, row in csv.iterrows():\n",
    "        #center\n",
    "        X.append(row['center'].strip())\n",
    "        y.append(row['steering'])\n",
    "        #left\n",
    "        X.append(row['left'].strip())\n",
    "        y.append(row['steering']+CAMERA_LEFT_RIGHT_OFFSET)\n",
    "        #right\n",
    "        X.append(row['right'].strip())\n",
    "        y.append(row['steering']-CAMERA_LEFT_RIGHT_OFFSET)\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.434873",
     "start_time": "2017-01-28T22:33:34.411824"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read Images\n",
    "\n",
    "def resize_and_normalize(img):\n",
    "    #printing out some stats and plotting\n",
    "    #print('This image is:', type(img), 'with dimesions:', img.shape)\n",
    "    #print(img)\n",
    "    #Cut Top and Bottom (sky and car)\n",
    "    #img_cut = img[IMAGE_CUT_TOP_HEIGHT:img.shape[0]-IMAGE_CUT_DOWN_HEIGHT, :, :]\n",
    "    \n",
    "    img_cut = img[IMAGE_CUT_TOP_HEIGHT:160-IMAGE_CUT_DOWN_HEIGHT, :, :]\n",
    "\n",
    "    #Resize to smaller image size\n",
    "    img_resize = cv2.resize(img_cut, (IMAGE_RESIZE_WIDTH, IMAGE_RESIZE_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "    #Normalizing to a range of -0.5 to +0.5\n",
    "    img_norm = (img_resize / 255. - .5).astype(np.float32)\n",
    "\n",
    "    return img_norm\n",
    "\n",
    "#image = cv2.imread(PATH_TRAIN_FOLDER+'IMG/center_2016_12_01_13_30_48_287.jpg')\n",
    "#plt.imshow(resize_image(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.490441",
     "start_time": "2017-01-28T22:33:34.443947"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augmentation(path, steering, validation):    \n",
    "\n",
    "    #Load\n",
    "    image = cv2.imread(path)\n",
    "    \n",
    "    #Augment\n",
    "    if not validation:\n",
    "        #Darken\n",
    "        if random.random() < CHANCES_DARKEN:\n",
    "            image = random_darken(image)\n",
    "\n",
    "        #Shift\n",
    "        if random.random() < CHANCES_SHIFT:\n",
    "            image = random_shift(image, 0, 0.2, 0, 1, 2)\n",
    "\n",
    "        #Flip\n",
    "        if random.random() < CHANCES_FLIP:\n",
    "            image = flip_axis(image,1)\n",
    "            steering = steering * -1    \n",
    "            \n",
    "    #Resize\n",
    "    image = resize_and_normalize(image)\n",
    "\n",
    "    \n",
    "    return image, steering\n",
    "    \n",
    "def random_darken(image):\n",
    "    \n",
    "    w = image.shape[0]\n",
    "    h = image.shape[1]\n",
    "    \n",
    "    # Convert the image to HSV\n",
    "    temp = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Compute a random brightness value and apply to the image\n",
    "    brightness = BRIGHTNESS_RANGE + np.random.uniform()\n",
    "    \n",
    "    # Create a random Box\n",
    "    x1, y1 = random.randint(0, w), random.randint(0, h)\n",
    "    x2, y2 = random.randint(x1, w), random.randint(y1, h)\n",
    "    for i in range(x1, x2):\n",
    "        for j in range(y1, y2):\n",
    "            temp[i,j, 2] = temp[i, j, 2] * brightness\n",
    "\n",
    "    # Convert back to RGB and return\n",
    "    return cv2.cvtColor(temp, cv2.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-28T22:33:34.526208",
     "start_time": "2017-01-28T22:33:34.499077"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "def model(load, shape, checkpoint=None):\n",
    "    \"\"\"Return a model from file or to train on.\"\"\"\n",
    "    if load and checkpoint: return load_model(checkpoint)\n",
    "\n",
    "    conv_layers, dense_layers = [32, 32, 64, 128], [1024, 512]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, activation='elu', input_shape=shape))\n",
    "    model.add(MaxPooling2D())\n",
    "    for cl in conv_layers:\n",
    "        model.add(Convolution2D(cl, 3, 3, activation='elu'))\n",
    "        model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    for dl in dense_layers:\n",
    "        model.add(Dense(dl, activation='elu'))\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #NVIDIA-Model\n",
    "# def model(load, shape, checkpoint=None):\n",
    "#     \"\"\"Return a model from file or to train on.\"\"\"\n",
    "#     if load and checkpoint: return load_model(checkpoint)\n",
    "\n",
    "#     conv_layers, dense_layers = [32, 32, 64, 128], [1024, 512]\n",
    "    \n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Convolution2D(32, 3, 3, activation='elu', input_shape=shape))\n",
    "#     model.add(MaxPooling2D())\n",
    "#     for cl in conv_layers:\n",
    "#         model.add(Convolution2D(cl, 3, 3, activation='elu'))\n",
    "#         model.add(MaxPooling2D())\n",
    "#     model.add(Flatten())\n",
    "#     for dl in dense_layers:\n",
    "#         model.add(Dense(dl, activation='elu'))\n",
    "#         model.add(Dropout(0.5))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(loss='mse', optimizer=\"adam\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-28T21:33:34.373Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _generator(batch_size, X, y, path, validation=False):\n",
    "    \"\"\"Generate batches of training data forever.\"\"\"\n",
    "    \n",
    "    while 1:\n",
    "        batch_X, batch_y = [], []\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            sample_index = random.randint(0, len(X) - 1)\n",
    "            sa = y[sample_index]       \n",
    "            \n",
    "            image, sa = augmentation(path+X[sample_index], sa, validation)\n",
    "            batch_X.append(image)\n",
    "            batch_y.append(sa)\n",
    "        yield np.array(batch_X), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(net):\n",
    "    net.save('model.h5')\n",
    "    \n",
    "    json_string = net.to_json()\n",
    "    with open('model.json', 'w') as outfile:\n",
    "        outfile.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net,X, y, path):\n",
    "\n",
    "    net.fit_generator(_generator(256, X, y, path), samples_per_epoch=21990, nb_epoch=8)\n",
    "    save_model(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(net,X, y, path):\n",
    "    return net.evaluate_generator(_generator(256, X, y, path, validation=True), val_samples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_drivinig():\n",
    "    #Build model\n",
    "    net = model(load=False, shape=(IMAGE_RESIZE_WIDTH, IMAGE_RESIZE_HEIGHT, 3))\n",
    "    \n",
    "    #Read data and train test split them\n",
    "    X,y = read_csv(PATH_TRAIN_FOLDER+FILENAME_CSV)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    #Training\n",
    "    train(net, X_train, y_train, PATH_TRAIN_FOLDER)\n",
    "    \n",
    "    #Evaluation - Testset\n",
    "    loss = evaluate(net, X_test, y_test, PATH_TRAIN_FOLDER)\n",
    "    print(\"Evaluation - Testset: {}\".format(loss))\n",
    "    \n",
    "    #Evaluation - Validation-Test#1\n",
    "    X_test,y_test = read_csv(PATH_VALIDATION1+FILENAME_CSV)\n",
    "    loss = evaluate(net, X_test, y_test, PATH_VALIDATION1)\n",
    "    print(\"Evaluation - Validation-Test#1: {}\".format(loss))\n",
    "        \n",
    "    #Evaluation - Validation-Test#2\n",
    "    X_test,y_test = read_csv(PATH_VALIDATION2+FILENAME_CSV)\n",
    "    loss = evaluate(net, X_test, y_test, PATH_VALIDATION2)\n",
    "    print(\"Evaluation - Validation-Test#2: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_checker():\n",
    "    print(\"*------- STANDARD ---------*\")\n",
    "    learn_drivinig()\n",
    "    \n",
    "    #Camera\n",
    "    CAMERA_LEFT_RIGHT_OFFSET = 0.1\n",
    "    print(\"*------- CAMERA_LEFT_RIGHT_OFFSET = 0.1 ---------*\")\n",
    "    learn_drivinig()\n",
    "    \n",
    "    #Camera\n",
    "    CAMERA_LEFT_RIGHT_OFFSET = 0.3\n",
    "    print(\"*------- CAMERA_LEFT_RIGHT_OFFSET = 0.3 ---------*\")\n",
    "    learn_drivinig()\n",
    "    \n",
    "    #resest\n",
    "    CAMERA_LEFT_RIGHT_OFFSET = 0.2\n",
    "    \n",
    "    #Image \n",
    "    IMAGE_CUT_TOP_HEIGHT = 65\n",
    "    IMAGE_CUT_DOWN_HEIGHT = 35 \n",
    "    print(\"*------- IMAGE_CUT_TOP_HEIGHT = 65 \\nIMAGE_CUT_DOWN_HEIGHT = 35  ---------*\")\n",
    "    learn_drivinig()\n",
    "    \n",
    "    #Image \n",
    "    IMAGE_CUT_TOP_HEIGHT = 45\n",
    "    IMAGE_CUT_DOWN_HEIGHT = 15 \n",
    "    print(\"*------- IMAGE_CUT_TOP_HEIGHT = 45 \\nIMAGE_CUT_DOWN_HEIGHT = 15 ---------*\")\n",
    "    learn_drivinig()  \n",
    "    \n",
    "    #reset\n",
    "    IMAGE_CUT_TOP_HEIGHT = 55\n",
    "    IMAGE_CUT_DOWN_HEIGHT = 25 \n",
    "    \n",
    "    IMAGE_RESIZE_WIDTH = 64\n",
    "    IMAGE_RESIZE_HEIGHT = 64\n",
    "    print(\"*------- IMAGE_RESIZE_WIDTH = 64 \\nIMAGE_RESIZE_HEIGHT = 64 ---------*\")\n",
    "    learn_drivinig()  \n",
    "    \n",
    "    IMAGE_RESIZE_WIDTH = 32\n",
    "    IMAGE_RESIZE_HEIGHT = 32\n",
    "    print(\"*------- IMAGE_RESIZE_WIDTH = 32 \\nIMAGE_RESIZE_HEIGHT = 32 ---------*\")\n",
    "    learn_drivinig()  \n",
    "    \n",
    "    #reset\n",
    "    IMAGE_RESIZE_WIDTH = 100\n",
    "    IMAGE_RESIZE_HEIGHT = 100\n",
    "    \n",
    "    #NoAugmentation\n",
    "    CHANCES_SHIFT = 0.0\n",
    "    CHANCES_FLIP = 0.0\n",
    "    CHANCES_DARKEN = 0.0\n",
    "    print(\"*------- No Augmentation ---------*\")\n",
    "    learn_drivinig()  \n",
    "    \n",
    "    #reset\n",
    "    CHANCES_SHIFT = 0.5\n",
    "    CHANCES_FLIP = 0.5\n",
    "    CHANCES_DARKEN = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-28T21:33:34.380Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "  512/21990 [..............................] - ETA: 609s - loss: 0.0437\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-c1e87688fa9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#parameter_checker()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlearn_drivinig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-2656b1dfce44>\u001b[0m in \u001b[0;36mlearn_drivinig\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH_TRAIN_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#Evaluation - Testset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-01d7abab339c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, X, y, path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21990\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1460\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1462\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weedjo/anaconda3/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':   \n",
    "    #parameter_checker()\n",
    "    learn_drivinig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('model.json', 'r') as jfile:\n",
    "          model = model_from_json(jfile.read())\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
